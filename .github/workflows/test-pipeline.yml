name: CastMatch Test Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

env:
  NODE_VERSION: '18.x'
  PYTHON_VERSION: '3.11'
  
jobs:
  # Code Quality Checks
  lint-and-format:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run ESLint
        run: npm run lint
      
      - name: Check Prettier formatting
        run: npm run format:check
      
      - name: TypeScript type check
        run: npx tsc --noEmit

  # Unit Tests
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: lint-and-format
    
    strategy:
      matrix:
        service: [backend, frontend, ai-service]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        if: matrix.service != 'ai-service'
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Setup Python
        if: matrix.service == 'ai-service'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies (Node)
        if: matrix.service != 'ai-service'
        run: |
          if [ "${{ matrix.service }}" == "frontend" ]; then
            cd frontend && npm ci
          else
            npm ci
          fi
      
      - name: Install dependencies (Python)
        if: matrix.service == 'ai-service'
        run: |
          cd python-ai-service
          pip install -r requirements.txt
          pip install -r requirements-test.txt
      
      - name: Run unit tests
        run: |
          if [ "${{ matrix.service }}" == "backend" ]; then
            npm run test:unit -- --coverage
          elif [ "${{ matrix.service }}" == "frontend" ]; then
            cd frontend && npm run test:unit -- --coverage
          else
            cd python-ai-service && pytest tests/unit --cov=src --cov-report=xml
          fi
      
      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage/lcov.info
          flags: ${{ matrix.service }}
          name: ${{ matrix.service }}-coverage

  # Integration Tests
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_USER: castmatch
          POSTGRES_PASSWORD: testpass
          POSTGRES_DB: castmatch_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: Install dependencies
        run: |
          npm ci
          cd python-ai-service
          pip install -r requirements.txt
      
      - name: Setup test database
        env:
          DATABASE_URL: postgresql://castmatch:testpass@localhost:5432/castmatch_test
        run: |
          npm run db:migrate
          npm run db:seed:test
      
      - name: Start services
        run: |
          # Start backend
          npm run start:test &
          
          # Start AI service
          cd python-ai-service
          uvicorn main:app --port 8000 &
          
          # Wait for services to be ready
          npx wait-on http://localhost:3001/health http://localhost:8000/health -t 30000
      
      - name: Run integration tests
        env:
          DATABASE_URL: postgresql://castmatch:testpass@localhost:5432/castmatch_test
          REDIS_URL: redis://localhost:6379
          AI_SERVICE_URL: http://localhost:8000
        run: npm run test:integration
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results
          path: coverage/integration/

  # E2E Tests
  e2e-tests:
    name: E2E Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    
    strategy:
      matrix:
        browser: [chromium, firefox, webkit]
        shard: [1, 2, 3]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: |
          npm ci
          cd frontend && npm ci
          npx playwright install --with-deps ${{ matrix.browser }}
      
      - name: Start Docker Compose services
        run: |
          docker-compose -f docker-compose.test.yml up -d
          ./scripts/wait-for-services.sh
      
      - name: Run E2E tests
        run: |
          npx playwright test \
            --browser=${{ matrix.browser }} \
            --shard=${{ matrix.shard }}/3 \
            --reporter=html
      
      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-results-${{ matrix.browser }}-${{ matrix.shard }}
          path: |
            test-results/
            playwright-report/
      
      - name: Upload screenshots on failure
        if: failure()
        uses: actions/upload-artifact@v3
        with:
          name: e2e-screenshots-${{ matrix.browser }}-${{ matrix.shard }}
          path: test-results/**/*.png

  # Performance Tests
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    needs: integration-tests
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install Artillery
        run: npm install -g artillery@latest
      
      - name: Start services with production config
        run: |
          docker-compose -f docker-compose.prod.yml up -d
          ./scripts/wait-for-services.sh
      
      - name: Run load tests
        run: |
          artillery run tests/performance/load-test.yml \
            --output tests/performance/results.json
      
      - name: Generate performance report
        run: |
          artillery report tests/performance/results.json \
            --output tests/performance/report.html
      
      - name: Check performance thresholds
        run: |
          node scripts/check-performance-thresholds.js tests/performance/results.json
      
      - name: Upload performance results
        uses: actions/upload-artifact@v3
        with:
          name: performance-results
          path: |
            tests/performance/results.json
            tests/performance/report.html
      
      - name: Comment PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('tests/performance/results.json'));
            const comment = `## Performance Test Results
            
            - **P95 Response Time**: ${results.aggregate.latency.p95}ms
            - **P99 Response Time**: ${results.aggregate.latency.p99}ms
            - **Requests/sec**: ${results.aggregate.rps.mean}
            - **Error Rate**: ${results.aggregate.errors.rate}%
            
            [View full report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

  # Security Tests
  security-tests:
    name: Security Tests
    runs-on: ubuntu-latest
    needs: unit-tests
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: npm ci
      
      - name: Run security tests
        run: npm run test:security
      
      - name: Run npm audit
        run: npm audit --audit-level=high
        continue-on-error: true
      
      - name: Run Snyk security scan
        uses: snyk/actions/node@master
        env:
          SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}
        with:
          args: --severity-threshold=high
      
      - name: Run OWASP dependency check
        uses: dependency-check/Dependency-Check_Action@main
        with:
          project: 'CastMatch'
          path: '.'
          format: 'HTML'
      
      - name: Upload security reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            dependency-check-report.html
            snyk-report.json

  # Accessibility Tests
  accessibility-tests:
    name: Accessibility Tests
    runs-on: ubuntu-latest
    needs: e2e-tests
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'
      
      - name: Install dependencies
        run: |
          npm ci
          cd frontend && npm ci
          npm install -g @axe-core/cli
      
      - name: Start frontend application
        run: |
          cd frontend
          npm run build
          npm run start &
          npx wait-on http://localhost:3000 -t 30000
      
      - name: Run axe accessibility tests
        run: |
          axe http://localhost:3000 \
            --tags wcag2aa \
            --save accessibility-report.json \
            --reporter json
      
      - name: Run Pa11y tests
        run: |
          npx pa11y http://localhost:3000 \
            --standard WCAG2AA \
            --reporter json > pa11y-report.json
      
      - name: Check accessibility violations
        run: node scripts/check-accessibility.js
      
      - name: Upload accessibility reports
        uses: actions/upload-artifact@v3
        with:
          name: accessibility-reports
          path: |
            accessibility-report.json
            pa11y-report.json

  # Test Coverage Report
  coverage-report:
    name: Generate Coverage Report
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests]
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all coverage reports
        uses: actions/download-artifact@v3
        with:
          path: coverage-reports
      
      - name: Merge coverage reports
        run: |
          npm install -g nyc
          nyc merge coverage-reports coverage/merged.json
          nyc report \
            --reporter=html \
            --reporter=text \
            --reporter=lcov \
            --temp-dir=coverage
      
      - name: Generate coverage badge
        uses: cicirello/jacoco-badge-generator@v2
        with:
          coverage-file: coverage/lcov.info
          badges-directory: coverage/badges
      
      - name: Upload final coverage report
        uses: codecov/codecov-action@v3
        with:
          file: coverage/lcov.info
          fail_ci_if_error: true
      
      - name: Comment coverage on PR
        if: github.event_name == 'pull_request'
        uses: romeovs/lcov-reporter-action@v0.3.1
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          lcov-file: coverage/lcov.info

  # Deploy to Staging (after all tests pass)
  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [
      unit-tests,
      integration-tests,
      e2e-tests,
      security-tests,
      accessibility-tests
    ]
    if: github.ref == 'refs/heads/develop'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Deploy to staging environment
        run: |
          echo "Deploying to staging..."
          # Add deployment scripts here
      
      - name: Run smoke tests
        run: |
          npm run test:smoke -- --env=staging
      
      - name: Notify team
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Staging deployment completed'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}

  # Quality Gates Check
  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    needs: [
      unit-tests,
      integration-tests,
      e2e-tests,
      performance-tests,
      security-tests,
      accessibility-tests,
      coverage-report
    ]
    if: always()
    
    steps:
      - name: Check quality gates
        run: |
          echo "Checking quality gates..."
          
          # Check if all required jobs passed
          if [[ "${{ needs.unit-tests.result }}" != "success" ]]; then
            echo "Unit tests failed"
            exit 1
          fi
          
          if [[ "${{ needs.integration-tests.result }}" != "success" ]]; then
            echo "Integration tests failed"
            exit 1
          fi
          
          if [[ "${{ needs.e2e-tests.result }}" != "success" ]]; then
            echo "E2E tests failed"
            exit 1
          fi
          
          if [[ "${{ needs.security-tests.result }}" != "success" ]]; then
            echo "Security tests failed"
            exit 1
          fi
          
          echo "All quality gates passed!"
      
      - name: Update PR status
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const status = '${{ job.status }}' === 'success' ? 'success' : 'failure';
            github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.payload.pull_request.head.sha,
              state: status,
              context: 'Quality Gates',
              description: status === 'success' ? 'All checks passed' : 'Some checks failed'
            });